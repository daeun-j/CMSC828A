{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 1. Ensenble Learning\n### 1.1. Confidence - based Selection \n* For each task model in task_list, the code calls the model with the test_example as arguments and stores the output in outputs.\n* The predicted class label is computed by getting the index of the highest value along the last dimension of the logits tensor in outputs.\n* The softmax probabilities of the output logits tensor in outputs are computed using torch.softmax.\n* The top k probabilities and their corresponding class labels are found using probabilities.topk.\n",
      "metadata": {
        "id": "1fRdb26Si9hs"
      }
    },
    {
      "cell_type": "code",
      "source": "def ensemble_prediction(test_example, \n                        task_models, \n                        aggregation_method=\"confidence\",\n                        knn_model=None):\n\n    if aggregation_method == \"confidence\":\n        confidence_score = []\n        i = 0\n        task_list = list(task_models.keys())\n        \n        for task in task_list:\n            outputs = task_models[task](**test_example)\n            prediction = outputs.logits.argmax(dim=-1)\n            probabilities = torch.softmax(outputs.logits, dim=-1)\n            pred_probs, pred_classes = probabilities.topk(k=1, dim=1)\n            conf_scores = pred_probs.squeeze().tolist() \n            confidence_score.append(conf_scores)\n        \n        max_conf = max(confidence_score)\n        max_conf_index = confidence_score.index(max_conf)\n       \n        for index, task in enumerate(task_list):  \n            if index == max_conf_index:\n                outputs = task_models[task](**test_example)\n                prediction = outputs.logits.argmax(dim=-1)",
      "metadata": {
        "id": "CZJp_lWSi9hv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# 1. Ensenble Learning\n### 1.2. Entropy - based Selection \n* For each task model in task_list, the code calls the model with the test_example as arguments and stores the output in outputs.\n* The predicted class label is computed by getting the index of the highest value along the last dimension of the logits tensor in outputs.\n* The softmax probabilities of the output logits tensor in outputs are computed using torch.softmax.\n* The top k probabilities and their corresponding class labels are found using probabilities.topk.\n* The entropy of the predicted probabilities is computed using the Shannon entropy formula and stored in entropies.\n* The best model index is found by getting the index of the minimum value in entropies using np.argmin.",
      "metadata": {
        "id": "6_rgTqXji9hx"
      }
    },
    {
      "cell_type": "code",
      "source": "    elif aggregation_method == \"entropy\":\n        entropies = []        \n        task_list = list(task_models.keys())\n        \n        for task in task_list:\n            outputs = task_models[task](**test_example)\n            prediction = outputs.logits.argmax(dim=-1)\n            probabilities = torch.softmax(outputs.logits, dim=-1)\n            pred_probs, pred_classes = probabilities.topk(k=1, dim=1)\n            entropy = -torch.sum(pred_probs.cpu() * torch.log2(pred_probs.cpu())).item()\n            entropies.append(entropy)\n            \n        best_model_index = np.argmin(entropies)\n        \n        for index, task in enumerate(task_list):  \n            if index == best_model_index:\n                outputs = task_models[task](**test_example)\n                prediction = outputs.logits.argmax(dim=-1)",
      "metadata": {
        "id": "F8Q6Ziwji9hy",
        "outputId": "01db1198-62a1-4c98-d230-691119fa5abf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-3-8e50843a2c7b>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    elif aggregation_method == \"entropy\":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### 1.2. Stacking\n\n* Firstly, 10% of training samples are reserved from each task split for training the linear classification head.\n\n* Linear classification head is initialized.\n\n* By loading the pretrained task models, which are trained by using the rest 90% of training samples, penultimate layer feature representations are obtained and are kept in \"task_features\" with reserved training samples.\n\n* These representations are concatenated in variable \"res\" to be fed into linear classification head for training.\n",
      "metadata": {
        "id": "I_hBYP0Hi9hz"
      }
    },
    {
      "cell_type": "code",
      "source": "## 1) Reserving 10% of training samples for classification head training\nreserved_locations = []\n\n#hyper parameter\n#########################\nreserved_percentage = 0.1\n#########################\n\ntask_train_datasets = {}\n\nfor k in task_to_class_list.keys():\n    \n    unreserved_locations = []\n    \n    for label in task_to_class_list[k]:\n        \n        #find locations of the current label\n        curr = np.where(np.asarray(dataset[\"train\"][\"labels\"]) == label)[0]\n        #shuffle\n        np.random.shuffle(curr)\n        #choose reserved_percentage amount of the shuffled data for head training\n        reserved_locations.extend( curr[ : int(len(curr) * reserved_percentage)])\n        #rest is unreserved\n        unreserved_locations.extend(curr[int(len(curr) * reserved_percentage) : ])\n    \n    task_train_datasets[k] = dataset[\"train\"].select(unreserved_locations)\n    \ntask_train_datasets = DatasetDict(task_train_datasets)\n\nreserved_task_train_datasets2 = dataset[\"train\"].select(reserved_locations)\n\n\n## 2) Initializing a linear classifier head with corresponding optimizer and loss function for classification\nclass Classifier(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(in_features,out_features),\n            nn.Softmax()\n            \n        )\n    def forward(self,x):\n        return self.classifier(x)\n\nmodel_classifier = Classifier(5*512,200)\noptimizer_classifier = torch.optim.AdamW(model_classifier.parameters(), lr = args.learning_rate, weight_decay= args.weight_decay)\ncriterion_classifier = nn.CrossEntropyLoss()\n\n\n## 3) Training newly initialized linear classifier head by using the concatenation of features from pretrained task models as input vector\n\nif not args.eval_only:\n      \n  train_dataloader = reserved_task_train_dataloaders2 # it should have all reserved training samples (shuffled)\n  completed_steps = 0\n  # total_train_step_this_task = 0 # not only for one task, for all tasks \n  # total_train_steps = sum(task_steps_per_epoch.values()) * args.num_train_epochs\n  total_train_steps = len(train_dataloader) * args.num_train_epochs\n  task_progress_bar = tqdm(range(total_train_steps), disable=not accelerator.is_local_main_process, desc=\" Classifier head training\")\n  model_classifier.to(accelerator.device)\n  for epoch in range(args.num_train_epochs):\n      model_classifier.train()\n      if args.with_tracking:\n          total_loss = 0\n      for step, batch in enumerate(train_dataloader):\n          # task_features = {task: [] for task in reserved_task_train_dataloaders}\n          task_features = {task: [] for task in task_train_datasets.keys()}\n          # pdb.set_trace()\n          # same sample should pass over all trained task models\n\n          for task in task_models:\n              task_models[task] = accelerator.unwrap_model(task_models[task])\n              task_models[task].load_state_dict(torch.load(f\"{args.output_dir}/task_{task}/pytorch_model.bin\"))\n              task_models[task].to(accelerator.device)\n              task_models[task].eval()\n              \n              model = task_models[task]\n              # model.to(accelerator.device)\n              feature_vector = model.resnet(batch['pixel_values']) # output at the end of encoder\n              feature_vector = model.classifier[0] (feature_vector['pooler_output']) # flattened\n              task_features[task].append(feature_vector) # append feature vectors\n              # model.to(\"cpu\") \n          \n          \n          # concatenate feature representations for each task model \n          \n          res = reduce(lambda x,y: x+y, task_features.values()) #list of Tensors [batch,512]\n          res = torch.stack(res) #[#task, batch, 512]\n          res = res.permute(1,0,2) #[batch, #task, 512]\n          res = res.reshape(len(res), len(res[0])*len(res[0,0])) #[batch, #task*512]\n          \n          y_pred = model_classifier(res)\n          loss = criterion_classifier(y_pred, batch['labels'])\n          if args.with_tracking:\n              total_loss += loss.detach().float()\n          accelerator.backward(loss)\n          optimizer_classifier.step()\n          optimizer_classifier.zero_grad()",
      "metadata": {
        "id": "do7odksji9hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# 2. Federate Learning\n# data/utils/partition/dirichlet.py\n * Client's Data Split Code\n * Odd-numbered clients are allocated 10 times more data than even-numbered clients.\n * data_indices : list of integers",
      "metadata": {
        "id": "t3FiW0fei9h0"
      }
    },
    {
      "cell_type": "code",
      "source": "from collections import Counter\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport math\n\n\ndef dirichlet(\n    ori_dataset: Dataset, num_clients: int, alpha: float, least_samples: int\n) -> Tuple[List[List[int]], Dict]:\n    num_classes = len(ori_dataset.classes)\n    min_size = 0\n    stats = {}\n    partition = {\"separation\": None, \"data_indices\": None}\n\n    targets_numpy = np.array(ori_dataset.targets, dtype=np.int32)\n    idx = [np.where(targets_numpy == i)[0] for i in range(num_classes)]\n\n    while min_size < least_samples:\n        data_indices = [[] for _ in range(num_clients)]\n        for k in range(num_classes):\n            np.random.shuffle(idx[k])\n            distrib = np.random.dirichlet(np.repeat(alpha, num_clients))\n            distrib = np.array(\n                [\n                    p * (len(idx_j) < len(targets_numpy) / num_clients)\n                    for p, idx_j in zip(distrib, data_indices)\n                ]\n            )\n            distrib = distrib / distrib.sum()\n            distrib = (np.cumsum(distrib) * len(idx[k])).astype(int)[:-1]\n            data_indices = [\n                np.concatenate((idx_j, idx.tolist())).astype(np.int64)\n                for idx_j, idx in zip(data_indices, np.split(idx[k], distrib))\n            ]\n            min_size = min([len(idx_j) for idx_j in data_indices])\n    \n    \n    data_indices =  [ i[math.floor(len(i)* 0.9 ):] if idx % 2 == 0 else i for idx, i in enumerate(data_indices)]\n    with open(\"file.txt\", \"w\") as f:\n        for s in data_indices:\n            f.write(str(s) +\"\\n\")\n            \n    for i in range(num_clients):\n        stats[i] = {\"x\": None, \"y\": None}\n        stats[i][\"x\"] = len(targets_numpy[data_indices[i]])\n        stats[i][\"y\"] = Counter(targets_numpy[data_indices[i]].tolist())\n\n    num_samples = np.array(list(map(lambda stat_i: stat_i[\"x\"], stats.values())))\n    stats[\"sample per client\"] = {\n        \"std\": num_samples.mean(),\n        \"stddev\": num_samples.std(),\n    }\n\n    partition[\"data_indices\"] = data_indices\n\n    return partition, stats\n",
      "metadata": {
        "id": "_zgUmsbIi9h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# src/config/args.py\n * Newly added parameters \n * self.args.lmb represents the lambda parameter vector in FedProx",
      "metadata": {
        "id": "Wnd9LCqMi9h2"
      }
    },
    {
      "cell_type": "code",
      "source": "\ndef get_fedavg_argparser() -> ArgumentParser:\n    parser = ArgumentParser()\n    parser.add_argument(\"-prox_lambda\", type=int, default=0)\n\ndef get_fedavgm_argparser() -> ArgumentParser:\n    parser = get_fedavg_argparser()\n    parser.add_argument(\"--server_momentum\", type=float, default=0.9)\n    return parser\n\ndef get_fedprox_argparser() -> ArgumentParser:\n    parser = get_fedavg_argparser()\n    parser.add_argument(\"--mu\", type=float, default=1.0)\n    parser.add_argument(\"--lmb\", type=list, default=[])\n    return parser",
      "metadata": {
        "id": "wN1OJzC_i9h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# src/client/fedprox.py\n * each client have different lambda\n",
      "metadata": {
        "id": "BUHTGxr6i9h4"
      }
    },
    {
      "cell_type": "code",
      "source": "from fedavg import FedAvgClient\nfrom src.config.utils import trainable_params\nimport numpy as np\nimport math\nimport torch \n\nclass FedProxClient(FedAvgClient):\n    def __init__(self, model, args, logger):\n        super(FedProxClient, self).__init__(model, args, logger)\n\n    def train(self, client_id, new_parameters, verbose=False):\n        delta, _, stats = super().train(\n            client_id, new_parameters, return_diff=True, verbose=verbose\n        )\n        self.client_id = client_id\n\n        # FedProx's model aggregation doesn't need weight\n        return delta, self.args.lmb[self.client_id], stats\n\n    \n    def fit(self):\n        self.model.train()\n        global_params = [p.clone().detach() for p in trainable_params(self.model)]\n        for i in range(self.local_epoch):\n            for x, y in self.trainloader:\n                if len(x) <= 1:\n                    continue\n\n                x, y = x.to(self.device), y.to(self.device)\n                logit = self.model(x)\n                loss = self.criterion(logit, y)\n                self.optimizer.zero_grad()\n                loss.backward()\n                for w, w_t in zip(trainable_params(self.model), global_params):\n                    w.grad.data += self.args.lmb[self.client_id] * (w.data - w_t.data)\n                self.optimizer.step()\n\n",
      "metadata": {
        "id": "7k_M6JVqi9h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "src/server/fedavg.py\n * The way to assign the lambdas to the clients\n * Defines how fedprox allocates lambdas\n    * self.args.prox_lambda == 1 : proportional to the number of clients dataset\n    * self.args.prox_lambda == 2 : inversly proportional to the number of clients dataset\n    * self.args.prox_lambda == else(usually 0) : 1\n * to enhance the difference between the lambdas, self.args.lmb is set to the l2 norm of the lmb saure.",
      "metadata": {
        "id": "0DQjmmCii9h6"
      }
    },
    {
      "cell_type": "code",
      "source": "\nself.args.lmb = [None] * self.client_num_in_total\n## Prox lambda schemes\nif self.args.prox_lambda == 1:\n    for cid in range(self.client_num_in_total):\n        self.args.lmb[cid] = len(partition[\"data_indices\"][cid][\"train\"])\n    self.args.lmb = torch.FloatTensor(self.args.lmb)\n    self.args.lmb  = normalize(self.args.lmb**2, p=2.0, dim = 0)\nelif self.args.prox_lambda == 2:\n    for cid in range(self.client_num_in_total):\n        self.args.lmb[cid] = len(partition[\"data_indices\"][cid][\"train\"])\n    self.args.lmb = torch.FloatTensor(self.args.lmb)\n    self.args.lmb = 1/(self.args.lmb**2)\n    self.args.lmb  = normalize(self.args.lmb, p=2.0, dim = 0)\nelse:\n    for cid in range(self.client_num_in_total):\n        self.args.lmb[cid] = 1\n    self.args.lmb = torch.FloatTensor(self.args.lmb)\n",
      "metadata": {
        "id": "l8S4fCwRi9h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# src/server/fedavg.py\n * save clients stats at @self.table\n * save clients stats\n",
      "metadata": {
        "id": "nTpI893bi9h7"
      }
    },
    {
      "cell_type": "code",
      "source": "  \nself.table = []\n\ndef log_info(self):\n    for label in [\"train\", \"test\"]:\n        # In the `user` split, there is no test data held by train clients, so plotting is unnecessary.\n        if (label == \"train\" and self.args.eval_train) or (\n            label == \"test\"\n            and self.args.eval_test\n            and self.args.dataset_args[\"split\"] != \"user\"\n        ):\n            correct_before = torch.tensor(\n                [\n                    self.client_stats[c][self.current_epoch][\"before\"][\n                        f\"{label}_correct\"\n                    ]\n                    for c in self.selected_clients\n                ]\n            )\n            correct_after = torch.tensor(\n                [\n                    self.client_stats[c][self.current_epoch][\"after\"][\n                        f\"{label}_correct\"\n                    ]\n                    for c in self.selected_clients\n                ]\n            )\n            num_samples = torch.tensor(\n                [\n                    self.client_stats[c][self.current_epoch][\"before\"][\n                        f\"{label}_size\"\n                    ]\n                    for c in self.selected_clients\n                ]\n            )\n            acc_before = (\n                correct_before.sum(dim=-1, keepdim=True) / num_samples.sum() * 100.0\n            ).item()\n            acc_after = (\n                correct_after.sum(dim=-1, keepdim=True) / num_samples.sum() * 100.0\n            ).item()\n            self.metrics[f\"{label}_before\"].append(acc_before)\n            self.metrics[f\"{label}_after\"].append(acc_after)\n\n            for label in [\"test\"]:\n                table = [self.client_stats[c][self.current_epoch][\"after\"][f\"{label}_correct\"]/\n                        self.client_stats[c][self.current_epoch][\"before\"][f\"{label}_size\"] *100\n                            if c in self.selected_clients else 0 for c, elem in enumerate([0]*self.client_num_in_total)]\n                self.table.append(table)\n",
      "metadata": {
        "id": "wePjthLLi9h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# src/server/fedavg.py\n * save clients performance figures in self.args.save_metrics",
      "metadata": {
        "id": "rngOGIpRi9h8"
      }
    },
    {
      "cell_type": "code",
      "source": "def run(self):\n\n    if self.trainer is None:\n        raise RuntimeError(\n            \"Specify your unique trainer or set `default_trainer` as True.\"\n        )\n\n    if self.args.visible:\n        self.viz.close(win=self.viz_win_name)\n\n    self.train()\n\n    self.logger.log(\n        \"=\" * 20, self.algo, \"TEST RESULTS:\", \"=\" * 20, self.test_results\n    )\n    self.check_convergence()\n\n    # save log files\n    if not os.path.isdir(OUT_DIR / self.algo) and (\n        self.args.save_log or self.args.save_fig or self.args.save_metrics\n    ):\n        os.makedirs(OUT_DIR / self.algo, exist_ok=True)\n\n    if self.args.save_log:\n        self.logger.save_text(OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_log.html\")\n\n    if self.args.save_fig:\n        import matplotlib\n        from matplotlib import pyplot as plt\n\n        matplotlib.use(\"Agg\")\n        linestyle = {\n            \"test_before\": \"solid\",\n            \"test_after\": \"solid\",\n            \"train_before\": \"dotted\",\n            \"train_after\": \"dotted\",\n        }\n        for label, acc in self.metrics.items():\n            if len(acc) > 0:\n                plt.plot(acc, label=label, ls=linestyle[label])\n        plt.title(f\"{self.algo}_{self.args.dataset}\")\n        plt.ylim(0, 100)\n        plt.xlabel(\"Communication Rounds\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend()\n        plt.savefig(\n            OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}.jpeg\", bbox_inches=\"tight\"\n        )\n    if self.args.save_metrics:\n        import pandas as pd\n        import numpy as np\n\n        accuracies = []\n        labels = []\n        for label, acc in self.metrics.items():\n            if len(acc) > 0:\n                accuracies.append(np.array(acc).T)\n                labels.append(label)\n        pd.DataFrame(np.stack(accuracies, axis=1), columns=labels).to_csv(\n            OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_acc_metrics.csv\",\n            index=False,\n        )\n        pd.DataFrame(np.array(self.table[1:])).to_csv(\n                OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_client_acc_metrics.csv\",\n                index=False,\n            )\n        \n",
      "metadata": {
        "id": "o_2cdev3i9h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}