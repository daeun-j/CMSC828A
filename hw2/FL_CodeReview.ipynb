{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data/utils/partition/dirichlet.py\n",
    " * Client's Data Split Code\n",
    " * Odd-numbered clients are allocated 10 times more data than even-numbered clients.\n",
    " * data_indices : list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "\n",
    "def dirichlet(\n",
    "    ori_dataset: Dataset, num_clients: int, alpha: float, least_samples: int\n",
    ") -> Tuple[List[List[int]], Dict]:\n",
    "    num_classes = len(ori_dataset.classes)\n",
    "    min_size = 0\n",
    "    stats = {}\n",
    "    partition = {\"separation\": None, \"data_indices\": None}\n",
    "\n",
    "    targets_numpy = np.array(ori_dataset.targets, dtype=np.int32)\n",
    "    idx = [np.where(targets_numpy == i)[0] for i in range(num_classes)]\n",
    "\n",
    "    while min_size < least_samples:\n",
    "        data_indices = [[] for _ in range(num_clients)]\n",
    "        for k in range(num_classes):\n",
    "            np.random.shuffle(idx[k])\n",
    "            distrib = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "            distrib = np.array(\n",
    "                [\n",
    "                    p * (len(idx_j) < len(targets_numpy) / num_clients)\n",
    "                    for p, idx_j in zip(distrib, data_indices)\n",
    "                ]\n",
    "            )\n",
    "            distrib = distrib / distrib.sum()\n",
    "            distrib = (np.cumsum(distrib) * len(idx[k])).astype(int)[:-1]\n",
    "            data_indices = [\n",
    "                np.concatenate((idx_j, idx.tolist())).astype(np.int64)\n",
    "                for idx_j, idx in zip(data_indices, np.split(idx[k], distrib))\n",
    "            ]\n",
    "            min_size = min([len(idx_j) for idx_j in data_indices])\n",
    "    \n",
    "    \n",
    "    data_indices =  [ i[math.floor(len(i)* 0.9 ):] if idx % 2 == 0 else i for idx, i in enumerate(data_indices)]\n",
    "    with open(\"file.txt\", \"w\") as f:\n",
    "        for s in data_indices:\n",
    "            f.write(str(s) +\"\\n\")\n",
    "            \n",
    "    for i in range(num_clients):\n",
    "        stats[i] = {\"x\": None, \"y\": None}\n",
    "        stats[i][\"x\"] = len(targets_numpy[data_indices[i]])\n",
    "        stats[i][\"y\"] = Counter(targets_numpy[data_indices[i]].tolist())\n",
    "\n",
    "    num_samples = np.array(list(map(lambda stat_i: stat_i[\"x\"], stats.values())))\n",
    "    stats[\"sample per client\"] = {\n",
    "        \"std\": num_samples.mean(),\n",
    "        \"stddev\": num_samples.std(),\n",
    "    }\n",
    "\n",
    "    partition[\"data_indices\"] = data_indices\n",
    "\n",
    "    return partition, stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/config/args.py\n",
    " * Newly added parameters \n",
    " * self.args.lmb represents the lambda parameter vector in FedProx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fedavg_argparser() -> ArgumentParser:\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-prox_lambda\", type=int, default=0)\n",
    "\n",
    "def get_fedavgm_argparser() -> ArgumentParser:\n",
    "    parser = get_fedavg_argparser()\n",
    "    parser.add_argument(\"--server_momentum\", type=float, default=0.9)\n",
    "    return parser\n",
    "\n",
    "def get_fedprox_argparser() -> ArgumentParser:\n",
    "    parser = get_fedavg_argparser()\n",
    "    parser.add_argument(\"--mu\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--lmb\", type=list, default=[])\n",
    "    return parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/client/fedprox.py\n",
    " * each client have different lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedavg import FedAvgClient\n",
    "from src.config.utils import trainable_params\n",
    "import numpy as np\n",
    "import math\n",
    "import torch \n",
    "\n",
    "class FedProxClient(FedAvgClient):\n",
    "    def __init__(self, model, args, logger):\n",
    "        super(FedProxClient, self).__init__(model, args, logger)\n",
    "\n",
    "    def train(self, client_id, new_parameters, verbose=False):\n",
    "        delta, _, stats = super().train(\n",
    "            client_id, new_parameters, return_diff=True, verbose=verbose\n",
    "        )\n",
    "        self.client_id = client_id\n",
    "\n",
    "        # FedProx's model aggregation doesn't need weight\n",
    "        return delta, self.args.lmb[self.client_id], stats\n",
    "\n",
    "    \n",
    "    def fit(self):\n",
    "        self.model.train()\n",
    "        global_params = [p.clone().detach() for p in trainable_params(self.model)]\n",
    "        for i in range(self.local_epoch):\n",
    "            for x, y in self.trainloader:\n",
    "                if len(x) <= 1:\n",
    "                    continue\n",
    "\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logit = self.model(x)\n",
    "                loss = self.criterion(logit, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                for w, w_t in zip(trainable_params(self.model), global_params):\n",
    "                    w.grad.data += self.args.lmb[self.client_id] * (w.data - w_t.data)\n",
    "                self.optimizer.step()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src/server/fedavg.py\n",
    " * The way to assign the lambdas to the clients\n",
    " * Defines how fedprox allocates lambdas\n",
    "    * self.args.prox_lambda == 1 : proportional to the number of clients dataset\n",
    "    * self.args.prox_lambda == 2 : inversly proportional to the number of clients dataset\n",
    "    * self.args.prox_lambda == else(usually 0) : 1\n",
    " * to enhance the difference between the lambdas, self.args.lmb is set to the l2 norm of the lmb saure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self.args.lmb = [None] * self.client_num_in_total\n",
    "## Prox lambda schemes\n",
    "if self.args.prox_lambda == 1:\n",
    "    for cid in range(self.client_num_in_total):\n",
    "        self.args.lmb[cid] = len(partition[\"data_indices\"][cid][\"train\"])\n",
    "    self.args.lmb = torch.FloatTensor(self.args.lmb)\n",
    "    self.args.lmb  = normalize(self.args.lmb**2, p=2.0, dim = 0)\n",
    "elif self.args.prox_lambda == 2:\n",
    "    for cid in range(self.client_num_in_total):\n",
    "        self.args.lmb[cid] = len(partition[\"data_indices\"][cid][\"train\"])\n",
    "    self.args.lmb = torch.FloatTensor(self.args.lmb)\n",
    "    self.args.lmb = 1/(self.args.lmb**2)\n",
    "    self.args.lmb  = normalize(self.args.lmb, p=2.0, dim = 0)\n",
    "else:\n",
    "    for cid in range(self.client_num_in_total):\n",
    "        self.args.lmb[cid] = 1\n",
    "    self.args.lmb = torch.FloatTensor(self.args.lmb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/server/fedavg.py\n",
    " * save clients stats at @self.table\n",
    " * save clients stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "self.table = []\n",
    "\n",
    "def log_info(self):\n",
    "    for label in [\"train\", \"test\"]:\n",
    "        # In the `user` split, there is no test data held by train clients, so plotting is unnecessary.\n",
    "        if (label == \"train\" and self.args.eval_train) or (\n",
    "            label == \"test\"\n",
    "            and self.args.eval_test\n",
    "            and self.args.dataset_args[\"split\"] != \"user\"\n",
    "        ):\n",
    "            correct_before = torch.tensor(\n",
    "                [\n",
    "                    self.client_stats[c][self.current_epoch][\"before\"][\n",
    "                        f\"{label}_correct\"\n",
    "                    ]\n",
    "                    for c in self.selected_clients\n",
    "                ]\n",
    "            )\n",
    "            correct_after = torch.tensor(\n",
    "                [\n",
    "                    self.client_stats[c][self.current_epoch][\"after\"][\n",
    "                        f\"{label}_correct\"\n",
    "                    ]\n",
    "                    for c in self.selected_clients\n",
    "                ]\n",
    "            )\n",
    "            num_samples = torch.tensor(\n",
    "                [\n",
    "                    self.client_stats[c][self.current_epoch][\"before\"][\n",
    "                        f\"{label}_size\"\n",
    "                    ]\n",
    "                    for c in self.selected_clients\n",
    "                ]\n",
    "            )\n",
    "            acc_before = (\n",
    "                correct_before.sum(dim=-1, keepdim=True) / num_samples.sum() * 100.0\n",
    "            ).item()\n",
    "            acc_after = (\n",
    "                correct_after.sum(dim=-1, keepdim=True) / num_samples.sum() * 100.0\n",
    "            ).item()\n",
    "            self.metrics[f\"{label}_before\"].append(acc_before)\n",
    "            self.metrics[f\"{label}_after\"].append(acc_after)\n",
    "\n",
    "            for label in [\"test\"]:\n",
    "                table = [self.client_stats[c][self.current_epoch][\"after\"][f\"{label}_correct\"]/\n",
    "                        self.client_stats[c][self.current_epoch][\"before\"][f\"{label}_size\"] *100\n",
    "                            if c in self.selected_clients else 0 for c, elem in enumerate([0]*self.client_num_in_total)]\n",
    "                self.table.append(table)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/server/fedavg.py\n",
    " * save clients performance figures in self.args.save_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "\n",
    "    if self.trainer is None:\n",
    "        raise RuntimeError(\n",
    "            \"Specify your unique trainer or set `default_trainer` as True.\"\n",
    "        )\n",
    "\n",
    "    if self.args.visible:\n",
    "        self.viz.close(win=self.viz_win_name)\n",
    "\n",
    "    self.train()\n",
    "\n",
    "    self.logger.log(\n",
    "        \"=\" * 20, self.algo, \"TEST RESULTS:\", \"=\" * 20, self.test_results\n",
    "    )\n",
    "    self.check_convergence()\n",
    "\n",
    "    # save log files\n",
    "    if not os.path.isdir(OUT_DIR / self.algo) and (\n",
    "        self.args.save_log or self.args.save_fig or self.args.save_metrics\n",
    "    ):\n",
    "        os.makedirs(OUT_DIR / self.algo, exist_ok=True)\n",
    "\n",
    "    if self.args.save_log:\n",
    "        self.logger.save_text(OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_log.html\")\n",
    "\n",
    "    if self.args.save_fig:\n",
    "        import matplotlib\n",
    "        from matplotlib import pyplot as plt\n",
    "\n",
    "        matplotlib.use(\"Agg\")\n",
    "        linestyle = {\n",
    "            \"test_before\": \"solid\",\n",
    "            \"test_after\": \"solid\",\n",
    "            \"train_before\": \"dotted\",\n",
    "            \"train_after\": \"dotted\",\n",
    "        }\n",
    "        for label, acc in self.metrics.items():\n",
    "            if len(acc) > 0:\n",
    "                plt.plot(acc, label=label, ls=linestyle[label])\n",
    "        plt.title(f\"{self.algo}_{self.args.dataset}\")\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlabel(\"Communication Rounds\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}.jpeg\", bbox_inches=\"tight\"\n",
    "        )\n",
    "    if self.args.save_metrics:\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        accuracies = []\n",
    "        labels = []\n",
    "        for label, acc in self.metrics.items():\n",
    "            if len(acc) > 0:\n",
    "                accuracies.append(np.array(acc).T)\n",
    "                labels.append(label)\n",
    "        pd.DataFrame(np.stack(accuracies, axis=1), columns=labels).to_csv(\n",
    "            OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_acc_metrics.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "        pd.DataFrame(np.array(self.table[1:])).to_csv(\n",
    "                OUT_DIR / self.algo / f\"{self.args.dataset}_gr{self.args.global_epoch}_le{self.args.local_epoch}_{self.args.model}_{self.args.prox_lambda}_client_acc_metrics.csv\",\n",
    "                index=False,\n",
    "            )\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
